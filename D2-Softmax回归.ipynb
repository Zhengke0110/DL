{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d297b3d1",
   "metadata": {},
   "source": [
    "# Softmax 回归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb795e",
   "metadata": {},
   "source": [
    "## 第一部分：Softmax 回归基础理论\n",
    "\n",
    "### 1.1 什么是分类问题？\n",
    "\n",
    "**回归 vs 分类**：\n",
    "\n",
    "- **回归问题**：预测连续的数值，比如房价预测（房价可以是任意实数）\n",
    "- **分类问题**：预测离散的类别，比如图像识别（一张图片要么是猫，要么是狗，要么是鸡）\n",
    "\n",
    "**分类问题的例子**：\n",
    "\n",
    "- 邮件是否为垃圾邮件（二分类）\n",
    "- 图像中的物体是什么（多分类）\n",
    "- 用户可能喜欢哪类电影（多分类）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed1957",
   "metadata": {},
   "source": [
    "### 1.2 重要专有名词解释\n",
    "\n",
    "#### 1.2.1 独热编码（One-hot Encoding）\n",
    "\n",
    "**通俗解释**：想象你在餐厅点菜，菜单上有 10 道菜，你只能选一道。独热编码就像是一个长度为 10 的向量，只有你选择的那道菜对应的位置是 1，其他位置都是 0。\n",
    "\n",
    "**数学表示**：\n",
    "\n",
    "- 如果有 3 个类别：猫、狗、鸡\n",
    "- 猫：$[1, 0, 0]$\n",
    "- 狗：$[0, 1, 0]$\n",
    "- 鸡：$[0, 0, 1]$\n",
    "\n",
    "**为什么要用独热编码**：因为类别之间没有大小关系。如果用数字 1、2、3 表示，会让模型误认为\"鸡\"比\"猫\"大，这是不合理的。\n",
    "\n",
    "#### 1.2.2 Softmax 函数\n",
    "\n",
    "**通俗解释**：Softmax 就像是一个\"概率分配器\"。给定一组分数，它能将这些分数转换为概率，使得：\n",
    "\n",
    "1. 所有概率都是正数\n",
    "2. 所有概率加起来等于 1\n",
    "\n",
    "**数学公式**：\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "其中 $x_i$ 是第 $i$ 个类别的原始分数，$n$ 是类别总数。\n",
    "\n",
    "**举例**：假设模型对一张图片给出原始分数 $[2.0, 1.0, 0.1]$\n",
    "\n",
    "- 经过 Softmax 后变成概率：$[0.66, 0.24, 0.10]$\n",
    "- 解释：66%可能是第 1 类，24%可能是第 2 类，10%可能是第 3 类\n",
    "\n",
    "#### 1.2.3 交叉熵损失（Cross-Entropy Loss）\n",
    "\n",
    "**通俗解释**：交叉熵损失衡量的是\"我们的预测与真实答案有多不一致\"。\n",
    "\n",
    "- 如果预测完全正确，损失为 0\n",
    "- 如果预测完全错误，损失会很大\n",
    "\n",
    "**数学公式**：\n",
    "\n",
    "$$\\text{CrossEntropy} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $y_i$ 是真实标签（独热编码）\n",
    "- $\\hat{y}_i$ 是预测概率\n",
    "- $n$ 是类别数量\n",
    "\n",
    "**直观理解**：假设真实答案是\"猫\"，我们预测：\n",
    "\n",
    "- 90%是猫，10%是狗 → 损失较小（预测基本正确）\n",
    "- 10%是猫，90%是狗 → 损失很大（预测很错误）\n",
    "\n",
    "#### 1.2.4 仿射变换（Affine Transformation）\n",
    "\n",
    "**通俗解释**：仿射变换就是\"线性变换 + 平移\"，数学公式是：\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\mathbf{W}$ 是权重矩阵（控制如何组合输入特征）\n",
    "- $\\mathbf{b}$ 是偏置向量（控制输出的基准值）\n",
    "- 这就像是加权求和再加上一个常数\n",
    "\n",
    "**举例**：计算考试总分\n",
    "\n",
    "$$\\text{总分} = \\text{数学成绩} \\times 0.4 + \\text{英语成绩} \\times 0.6 + 10$$\n",
    "\n",
    "这里 0.4、0.6 是权重，10 是偏置（平时分）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbeb928",
   "metadata": {},
   "source": [
    "### 1.3 Softmax 回归的网络架构\n",
    "\n",
    "#### 1.3.1 整体架构理解\n",
    "\n",
    "Softmax 回归是一个**单层神经网络**，专门用于多分类问题。\n",
    "\n",
    "**架构组成**：\n",
    "\n",
    "```\n",
    "输入层 → 全连接层（仿射变换） → Softmax函数 → 输出概率\n",
    "```\n",
    "\n",
    "**数学表示**：\n",
    "\n",
    "$$\\mathbf{o} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$$\n",
    "$$\\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{o})$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^d$ 是输入特征向量\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{q \\times d}$ 是权重矩阵\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^q$ 是偏置向量\n",
    "- $\\mathbf{o} \\in \\mathbb{R}^q$ 是线性变换的输出（logits）\n",
    "- $\\hat{\\mathbf{y}} \\in \\mathbb{R}^q$ 是最终的概率分布\n",
    "\n",
    "**具体过程**：\n",
    "\n",
    "1. **输入**：特征向量（如图像的像素值）\n",
    "2. **全连接层**：通过权重矩阵和偏置进行线性变换\n",
    "3. **Softmax**：将线性变换的结果转换为概率分布\n",
    "4. **输出**：每个类别的概率\n",
    "\n",
    "#### 1.3.2 参数数量计算\n",
    "\n",
    "**重要概念**：全连接层的参数开销\n",
    "\n",
    "对于输入维度 $d$、输出类别数 $q$ 的全连接层：\n",
    "\n",
    "- **权重参数**：$d \\times q$ 个\n",
    "- **偏置参数**：$q$ 个\n",
    "- **总参数**：$d \\times q + q = q(d + 1)$ 个\n",
    "\n",
    "**数学表示**：\n",
    "\n",
    "$$\\text{参数总数} = |\\mathbf{W}| + |\\mathbf{b}| = d \\times q + q = q(d + 1)$$\n",
    "\n",
    "**举例**：Fashion-MNIST 图像分类\n",
    "\n",
    "- 输入：$28 \\times 28 = 784$ 个像素\n",
    "- 输出：$10$ 个类别\n",
    "- 参数总数：$784 \\times 10 + 10 = 7850$ 个参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc32249",
   "metadata": {},
   "source": [
    "### 1.4 Softmax 函数的数学原理\n",
    "\n",
    "#### 1.4.1 为什么需要 Softmax？\n",
    "\n",
    "**问题**：线性层的输出可能是任意实数，但我们需要概率：\n",
    "\n",
    "- 概率必须是非负数\n",
    "- 所有概率之和必须等于 1\n",
    "\n",
    "**解决方案**：Softmax 函数\n",
    "\n",
    "#### 1.4.2 Softmax 公式解释\n",
    "\n",
    "**完整的 Softmax 公式**：\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{o})_j = \\frac{\\exp(o_j)}{\\sum_{k=1}^{q} \\exp(o_k)}$$\n",
    "\n",
    "或者用向量形式表示：\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{o}) = \\frac{\\exp(\\mathbf{o})}{\\sum_{k=1}^{q} \\exp(o_k)}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\mathbf{o} = [o_1, o_2, \\ldots, o_q]^T$ 是线性层的输出（logits）\n",
    "- $q$ 是类别总数\n",
    "- $\\exp(\\mathbf{o}) = [\\exp(o_1), \\exp(o_2), \\ldots, \\exp(o_q)]^T$\n",
    "\n",
    "**逐步理解**：\n",
    "\n",
    "1. **第一步**：对每个输出 $o_j$ 取指数 $\\exp(o_j)$\n",
    "   - 作用：确保所有值都是正数\n",
    "2. **第二步**：计算所有指数的和作为分母\n",
    "   - 作用：作为归一化常数\n",
    "3. **第三步**：每个指数值除以总和\n",
    "   - 作用：确保所有概率和为 1\n",
    "\n",
    "**数值例子**：\n",
    "\n",
    "假设原始输出：$\\mathbf{o} = [2.0, 1.0, 0.1]$\n",
    "\n",
    "- 取指数：$\\exp(\\mathbf{o}) = [\\exp(2.0), \\exp(1.0), \\exp(0.1)] = [7.389, 2.718, 1.105]$\n",
    "- 求和：$\\sum_{k=1}^{3} \\exp(o_k) = 7.389 + 2.718 + 1.105 = 11.212$\n",
    "- Softmax 结果：$\\hat{\\mathbf{y}} = [0.659, 0.242, 0.099]$\n",
    "\n",
    "验证：$0.659 + 0.242 + 0.099 = 1.000$ ✓\n",
    "\n",
    "#### 1.4.3 Softmax 的重要性质\n",
    "\n",
    "1. **单调性**：原始分数越大，对应概率越大\n",
    "   $$o_i > o_j \\Rightarrow \\text{softmax}(\\mathbf{o})_i > \\text{softmax}(\\mathbf{o})_j$$\n",
    "\n",
    "2. **概率性**：输出是有效的概率分布\n",
    "\n",
    "   - $\\text{softmax}(\\mathbf{o})_j > 0$ 对所有 $j$\n",
    "   - $\\sum_{j=1}^{q} \\text{softmax}(\\mathbf{o})_j = 1$\n",
    "\n",
    "3. **可微性**：可以用梯度下降优化\n",
    "   $$\\frac{\\partial \\text{softmax}(\\mathbf{o})_j}{\\partial o_i} = \\text{softmax}(\\mathbf{o})_j(\\delta_{ij} - \\text{softmax}(\\mathbf{o})_i)$$\n",
    "\n",
    "   其中 $\\delta_{ij}$ 是 Kronecker delta 函数\n",
    "\n",
    "4. **数值稳定性**：实际计算时使用数值稳定的版本\n",
    "   $$\\text{softmax}(\\mathbf{o})_j = \\frac{\\exp(o_j - \\max(\\mathbf{o}))}{\\sum_{k=1}^{q} \\exp(o_k - \\max(\\mathbf{o}))}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712106d",
   "metadata": {},
   "source": [
    "## 第二部分：图像分类数据集\n",
    "\n",
    "### 2.1 Fashion-MNIST 数据集介绍\n",
    "\n",
    "#### 2.1.1 什么是 Fashion-MNIST？\n",
    "\n",
    "**Fashion-MNIST**是一个服装图像分类数据集，被设计来替代经典的 MNIST 手写数字数据集。\n",
    "\n",
    "**数据集特点**：\n",
    "\n",
    "- **图像尺寸**：28×28 像素\n",
    "- **颜色**：灰度图像（单通道）\n",
    "- **类别数量**：10 个服装类别\n",
    "- **训练样本**：60,000 张图像\n",
    "- **测试样本**：10,000 张图像\n",
    "\n",
    "#### 2.1.2 十个服装类别详解\n",
    "\n",
    "| 标签 | 英文名称    | 中文名称 | 说明        |\n",
    "| ---- | ----------- | -------- | ----------- |\n",
    "| 0    | T-shirt/top | T 恤     | 短袖上衣    |\n",
    "| 1    | Trouser     | 裤子     | 长裤        |\n",
    "| 2    | Pullover    | 套衫     | 套头衫      |\n",
    "| 3    | Dress       | 连衣裙   | 女装裙子    |\n",
    "| 4    | Coat        | 外套     | 大衣/外套   |\n",
    "| 5    | Sandal      | 凉鞋     | 夏季鞋类    |\n",
    "| 6    | Shirt       | 衬衫     | 正装衬衫    |\n",
    "| 7    | Sneaker     | 运动鞋   | 休闲鞋      |\n",
    "| 8    | Bag         | 包       | 手提包/背包 |\n",
    "| 9    | Ankle boot  | 短靴     | 踝靴        |\n",
    "\n",
    "#### 2.1.3 为什么选择 Fashion-MNIST？\n",
    "\n",
    "1. **适中难度**：比 MNIST 更有挑战性，比真实图像简单\n",
    "2. **标准化**：图像大小统一，便于模型训练\n",
    "3. **充足数据**：7 万张图像足够训练深度学习模型\n",
    "4. **多样性**：10 个类别涵盖不同类型的服装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2654314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from IPython import display\n",
    "\n",
    "print(\"PyTorch版本:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d3242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义批量大小，决定一次迭代处理的样本数\n",
    "batch_size = 256\n",
    "\n",
    "# 加载Fashion-MNIST数据集的训练和测试迭代器\n",
    "# train_iter和test_iter将用于在训练和测试过程中提供数据批次\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6777ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                   Fashion-MNIST 数据维度核心分析                   \n",
      "============================================================\n",
      "\n",
      "📊 数据集基本信息\n",
      "--------------------------------------------------\n",
      "批次大小 (Batch Size)    : 256\n",
      "训练批次数量             : 235\n",
      "测试批次数量             : 40\n",
      "训练样本总数             : 60,160\n",
      "测试样本总数             : 10,240\n",
      "\n",
      "📊 张量维度信息\n",
      "--------------------------------------------------\n",
      "🖼️  特征张量 X 形状       : torch.Size([256, 1, 28, 28])\n",
      "    维度解释             : [批次=256, 通道=1, 高=28, 宽=28]\n",
      "    单图像像素数         : 784\n",
      "    数据类型             : torch.float32\n",
      "    数值范围             : [0.000, 1.000]\n",
      "\n",
      "🏷️  标签张量 y 形状       : torch.Size([256])\n",
      "    维度解释             : [批次大小=256]\n",
      "    标签范围             : [0, 9]\n",
      "    类别总数             : 10\n",
      "\n",
      "📊 Softmax回归模型参数\n",
      "--------------------------------------------------\n",
      "输入维度 (展平后)        : 784\n",
      "输出维度 (类别数)        : 10\n",
      "权重矩阵 W 形状          : (784, 10)\n",
      "偏置向量 b 形状          : (10,)\n",
      "总参数数量               : 7,850\n",
      "\n",
      "📊 关键形状变换\n",
      "--------------------------------------------------\n",
      "原始图像形状             : torch.Size([256, 1, 28, 28])\n",
      "展平后形状               : torch.Size([256, 784])\n",
      "变换公式                 : X.view(batch_size, -1)\n",
      "适用场景                 : 全连接层输入\n",
      "\n",
      "📊 当前批次类别分布\n",
      "--------------------------------------------------\n",
      "类别ID  类别名称     样本数  占比\n",
      "-----------------------------------\n",
      "  0     T-shirt       23     9.0%\n",
      "  1     Trouser       11     4.3%\n",
      "  2     Pullover      21     8.2%\n",
      "  3     Dress         27    10.5%\n",
      "  4     Coat          40    15.6%\n",
      "  5     Sandal        35    13.7%\n",
      "  6     Shirt         23     9.0%\n",
      "  7     Sneaker       31    12.1%\n",
      "  8     Bag           21     8.2%\n",
      "  9     Boot          24     9.4%\n",
      "\n",
      "============================================================\n",
      "                         ✅ 数据维度分析完成                         \n",
      "============================================================\n",
      "\n",
      "📊 数据集基本信息\n",
      "--------------------------------------------------\n",
      "批次大小 (Batch Size)    : 256\n",
      "训练批次数量             : 235\n",
      "测试批次数量             : 40\n",
      "训练样本总数             : 60,160\n",
      "测试样本总数             : 10,240\n",
      "\n",
      "📊 张量维度信息\n",
      "--------------------------------------------------\n",
      "🖼️  特征张量 X 形状       : torch.Size([256, 1, 28, 28])\n",
      "    维度解释             : [批次=256, 通道=1, 高=28, 宽=28]\n",
      "    单图像像素数         : 784\n",
      "    数据类型             : torch.float32\n",
      "    数值范围             : [0.000, 1.000]\n",
      "\n",
      "🏷️  标签张量 y 形状       : torch.Size([256])\n",
      "    维度解释             : [批次大小=256]\n",
      "    标签范围             : [0, 9]\n",
      "    类别总数             : 10\n",
      "\n",
      "📊 Softmax回归模型参数\n",
      "--------------------------------------------------\n",
      "输入维度 (展平后)        : 784\n",
      "输出维度 (类别数)        : 10\n",
      "权重矩阵 W 形状          : (784, 10)\n",
      "偏置向量 b 形状          : (10,)\n",
      "总参数数量               : 7,850\n",
      "\n",
      "📊 关键形状变换\n",
      "--------------------------------------------------\n",
      "原始图像形状             : torch.Size([256, 1, 28, 28])\n",
      "展平后形状               : torch.Size([256, 784])\n",
      "变换公式                 : X.view(batch_size, -1)\n",
      "适用场景                 : 全连接层输入\n",
      "\n",
      "📊 当前批次类别分布\n",
      "--------------------------------------------------\n",
      "类别ID  类别名称     样本数  占比\n",
      "-----------------------------------\n",
      "  0     T-shirt       23     9.0%\n",
      "  1     Trouser       11     4.3%\n",
      "  2     Pullover      21     8.2%\n",
      "  3     Dress         27    10.5%\n",
      "  4     Coat          40    15.6%\n",
      "  5     Sandal        35    13.7%\n",
      "  6     Shirt         23     9.0%\n",
      "  7     Sneaker       31    12.1%\n",
      "  8     Bag           21     8.2%\n",
      "  9     Boot          24     9.4%\n",
      "\n",
      "============================================================\n",
      "                         ✅ 数据维度分析完成                         \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Fashion-MNIST 数据维度分析\n",
    "def print_header(title):\n",
    "    \"\"\"打印格式化标题\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{title:^60}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"打印章节标题\"\"\"\n",
    "    print(f\"\\n📊 {title}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# 确保只执行一次分析\n",
    "print_header(\"Fashion-MNIST 数据维度核心分析\")\n",
    "\n",
    "# 获取一个批次数据进行分析\n",
    "X, y = next(iter(train_iter))\n",
    "\n",
    "# 1. 数据基本信息\n",
    "print_section(\"数据集基本信息\")\n",
    "print(f\"批次大小 (Batch Size)    : {batch_size}\")\n",
    "print(f\"训练批次数量             : {len(train_iter):,}\")\n",
    "print(f\"测试批次数量             : {len(test_iter):,}\")\n",
    "print(f\"训练样本总数             : {len(train_iter) * batch_size:,}\")\n",
    "print(f\"测试样本总数             : {len(test_iter) * batch_size:,}\")\n",
    "\n",
    "# 2. 张量维度核心信息\n",
    "print_section(\"张量维度信息\")\n",
    "print(f\"🖼️  特征张量 X 形状       : {X.shape}\")\n",
    "print(\n",
    "    f\"    维度解释             : [批次={X.shape[0]}, 通道={X.shape[1]}, 高={X.shape[2]}, 宽={X.shape[3]}]\"\n",
    ")\n",
    "print(f\"    单图像像素数         : {X.shape[1] * X.shape[2] * X.shape[3]:,}\")\n",
    "print(f\"    数据类型             : {X.dtype}\")\n",
    "print(f\"    数值范围             : [{X.min().item():.3f}, {X.max().item():.3f}]\")\n",
    "\n",
    "print(f\"\\n🏷️  标签张量 y 形状       : {y.shape}\")\n",
    "print(f\"    维度解释             : [批次大小={y.shape[0]}]\")\n",
    "print(f\"    标签范围             : [{y.min().item()}, {y.max().item()}]\")\n",
    "print(f\"    类别总数             : {len(torch.unique(y))}\")\n",
    "\n",
    "# 3. 模型参数维度\n",
    "print_section(\"Softmax回归模型参数\")\n",
    "X_flattened = X.view(X.shape[0], -1)  # 展平图像\n",
    "num_inputs = X_flattened.shape[1]  # 输入特征数\n",
    "num_outputs = len(torch.unique(y))  # 输出类别数\n",
    "\n",
    "print(f\"输入维度 (展平后)        : {num_inputs:,}\")\n",
    "print(f\"输出维度 (类别数)        : {num_outputs}\")\n",
    "print(f\"权重矩阵 W 形状          : ({num_inputs:,}, {num_outputs})\")\n",
    "print(f\"偏置向量 b 形状          : ({num_outputs},)\")\n",
    "print(f\"总参数数量               : {num_inputs * num_outputs + num_outputs:,}\")\n",
    "\n",
    "# 4. 形状变换演示\n",
    "print_section(\"关键形状变换\")\n",
    "print(f\"原始图像形状             : {X.shape}\")\n",
    "print(f\"展平后形状               : {X_flattened.shape}\")\n",
    "print(f\"变换公式                 : X.view(batch_size, -1)\")\n",
    "print(f\"适用场景                 : 全连接层输入\")\n",
    "\n",
    "# 5. 当前批次类别分布（简化版）\n",
    "print_section(\"当前批次类别分布\")\n",
    "label_counts = torch.bincount(y)\n",
    "labels = [\n",
    "    \"T-shirt\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Boot\",\n",
    "]\n",
    "\n",
    "print(\"类别ID  类别名称     样本数  占比\")\n",
    "print(\"-\" * 35)\n",
    "for i, count in enumerate(label_counts):\n",
    "    if count > 0:\n",
    "        percentage = count.item() / y.shape[0] * 100\n",
    "        print(f\"{i:^6}  {labels[i]:<10}  {count.item():^6}  {percentage:>4.1f}%\")\n",
    "\n",
    "# 结束标记\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ 数据维度分析完成\".center(60))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57273d",
   "metadata": {},
   "source": [
    "### 2.2 数据维度分析结果详解\n",
    "\n",
    "通过上面的代码分析，我们得到了 Fashion-MNIST 数据集的详细信息。让我逐一解释这些输出结果的含义：\n",
    "\n",
    "#### 📊 数据集基本信息解读\n",
    "\n",
    "**批次大小 (Batch Size): 256**\n",
    "\n",
    "- **含义**: 每次训练迭代处理 256 张图像\n",
    "- **作用**: 批次大小影响训练效率和内存使用\n",
    "- **为什么选择 256**: 这是一个常用的批次大小，平衡了计算效率和内存占用\n",
    "\n",
    "**训练批次数量: 235**\n",
    "\n",
    "- **计算**: 总训练样本 $\\div$ 批次大小 = $60,000 \\div 256 \\approx 235$\n",
    "- **含义**: 完整遍历一遍训练集需要 235 次迭代\n",
    "\n",
    "**测试批次数量: 40**\n",
    "\n",
    "- **计算**: 总测试样本 $\\div$ 批次大小 = $10,000 \\div 256 \\approx 40$\n",
    "- **含义**: 完整评估一遍测试集需要 40 次迭代\n",
    "\n",
    "#### 🖼️ 张量维度信息解读\n",
    "\n",
    "**特征张量 X 形状: $[256, 1, 28, 28]$**\n",
    "\n",
    "- **$[256]$**: 批次维度，表示同时处理 256 张图像\n",
    "- **$[1]$**: 通道维度，1 表示灰度图像（彩色图像为 3）\n",
    "- **$[28, 28]$**: 图像的高度和宽度，每张图像都是 $28 \\times 28$ 像素\n",
    "\n",
    "**单图像像素数: 784**\n",
    "\n",
    "- **计算**: $1 \\times 28 \\times 28 = 784$\n",
    "- **含义**: 每张图像包含 784 个像素值\n",
    "- **重要性**: 这将成为 Softmax 回归模型的输入特征数量\n",
    "\n",
    "**数据类型: torch.float32**\n",
    "\n",
    "- **含义**: 使用 32 位浮点数存储像素值\n",
    "- **范围**: 每个像素值占用 4 字节内存\n",
    "\n",
    "**数值范围: $[0.000, 1.000]$**\n",
    "\n",
    "- **含义**: 像素值已经归一化到 $[0,1]$ 之间\n",
    "- **好处**: 归一化有助于模型训练的稳定性和收敛速度\n",
    "\n",
    "#### 🏷️ 标签张量信息解读\n",
    "\n",
    "**标签张量 y 形状: $[256]$**\n",
    "\n",
    "- **含义**: 一维张量，包含 256 个标签值\n",
    "- **对应关系**: 每个标签对应批次中的一张图像\n",
    "\n",
    "**标签范围: $[0, 9]$**\n",
    "\n",
    "- **含义**: 标签值从 0 到 9，对应 10 个服装类别\n",
    "- **编码方式**: 使用整数编码而非独热编码（节省内存）\n",
    "\n",
    "#### ⚙️ Softmax 回归模型参数解读\n",
    "\n",
    "**输入维度 (展平后): 784**\n",
    "\n",
    "- **来源**: $28 \\times 28 = 784$ 个像素\n",
    "- **作用**: 每张图像被展平成 784 维的特征向量\n",
    "\n",
    "**输出维度 (类别数): 10**\n",
    "\n",
    "- **含义**: 10 个服装类别，模型需要输出 10 个概率值\n",
    "\n",
    "**权重矩阵 W 形状: $(784, 10)$**\n",
    "\n",
    "- **含义**: $784$ 行 $10$ 列的矩阵\n",
    "- **参数数量**: $784 \\times 10 = 7,840$ 个权重参数\n",
    "- **作用**: 将 784 维输入映射到 10 维输出\n",
    "\n",
    "**偏置向量 b 形状: $(10,)$**\n",
    "\n",
    "- **含义**: 10 个偏置值，每个输出类别对应一个\n",
    "- **作用**: 为每个类别提供基准调整\n",
    "\n",
    "**总参数数量: 7,850**\n",
    "\n",
    "- **计算**: $7,840\\text{（权重）} + 10\\text{（偏置）} = 7,850$\n",
    "- **意义**: 这是一个相对简单的模型，参数数量不大\n",
    "\n",
    "**数学公式表示**：\n",
    "\n",
    "$$\\text{总参数数量} = |\\mathbf{W}| + |\\mathbf{b}| = d \\times q + q = q(d + 1)$$\n",
    "\n",
    "其中：\n",
    "- $d = 784$（输入维度）\n",
    "- $q = 10$（输出类别数）\n",
    "\n",
    "#### 🔄 关键形状变换解读\n",
    "\n",
    "**原始图像形状 → 展平后形状**\n",
    "\n",
    "- **变换**: $[256, 1, 28, 28] \\rightarrow [256, 784]$\n",
    "- **目的**: 全连接层需要二维输入（批次 $\\times$ 特征）\n",
    "- **方法**: `X.view(batch_size, -1)` 自动计算展平维度\n",
    "\n",
    "**数学表示**：\n",
    "\n",
    "$$\\mathbf{X}_{\\text{flattened}} = \\text{reshape}(\\mathbf{X}, (\\text{batch\\_size}, -1))$$\n",
    "\n",
    "其中 $-1$ 表示自动计算该维度大小。\n",
    "\n",
    "#### 📈 当前批次类别分布解读\n",
    "\n",
    "这部分显示了当前批次中各类别的样本分布情况：\n",
    "\n",
    "**为什么分布不均匀？**\n",
    "\n",
    "- 数据加载器随机采样，每个批次的类别分布会有所不同\n",
    "- 这是正常现象，不同批次会有不同的类别组合\n",
    "\n",
    "**占比计算公式**：\n",
    "\n",
    "$$\\text{占比}_i = \\frac{\\text{类别}_i\\text{的样本数}}{\\text{批次总样本数}} \\times 100\\%$$\n",
    "\n",
    "**占比的重要性**：\n",
    "\n",
    "- 帮助了解数据的随机性\n",
    "- 检查是否存在严重的类别不平衡\n",
    "- 一般来说，每个类别在大批次中应该大致相等（约 $10\\%$）\n",
    "\n",
    "#### 💡 关键要点总结\n",
    "\n",
    "1. **数据规模**: $7$ 万张 $28 \\times 28$ 的灰度图像，分为 $10$ 个类别\n",
    "2. **模型简单**: Softmax 回归只有 $7,850$ 个参数，计算效率高\n",
    "3. **内存友好**: 每个批次约占用不到 $1\\text{MB}$ 内存\n",
    "4. **标准化**: 数据已预处理，像素值在 $[0,1]$ 范围内\n",
    "5. **维度变换**: 理解 $4\\text{D} \\rightarrow 2\\text{D}$ 的展平过程对后续模型构建很重要\n",
    "\n",
    "#### 🧮 内存占用详细计算\n",
    "\n",
    "**单张图像内存占用**：\n",
    "$$\\text{内存} = 1 \\times 28 \\times 28 \\times 4\\text{字节} = 3,136\\text{字节} = 3.06\\text{KB}$$\n",
    "\n",
    "**单批次图像内存占用**：\n",
    "$$\\text{内存} = 256 \\times 3.06\\text{KB} = 784\\text{KB} \\approx 0.77\\text{MB}$$\n",
    "\n",
    "**模型参数内存占用**：\n",
    "$$\\text{内存} = 7,850 \\times 4\\text{字节} = 31,400\\text{字节} = 30.7\\text{KB}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512a9ea",
   "metadata": {},
   "source": [
    "## 第三部分：Softmax 回归模型实现\n",
    "\n",
    "### 3.1 初始化模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a536e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入和输出的维度\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "# 初始化权重矩阵W，使用均值为0，标准差为0.01的正态分布随机数\n",
    "# 权重矩阵的尺寸由输入维度和输出维度决定\n",
    "# requires_grad设为True以启用梯度计算，用于反向传播\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "\n",
    "# 初始化偏置向量b，所有元素初始化为0\n",
    "# 偏置向量的长度等于输出维度\n",
    "# 同样启用梯度计算以支持反向传播\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7092c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化模型参数\n",
      "========================================\n",
      "权重矩阵 W 的形状: torch.Size([784, 10])\n",
      "偏置向量 b 的形状: torch.Size([10])\n",
      "总参数数量: 7850\n",
      "参数初始化: W用小随机数，b初始化为0\n"
     ]
    }
   ],
   "source": [
    "print(\"初始化模型参数\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"权重矩阵 W 的形状: {W.shape}\")\n",
    "print(f\"偏置向量 b 的形状: {b.shape}\")\n",
    "print(f\"总参数数量: {W.numel() + b.numel()}\")\n",
    "print(f\"参数初始化: W用小随机数，b初始化为0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657c5be",
   "metadata": {},
   "source": [
    "### 3.2 模型参数初始化结果详解\n",
    "\n",
    "通过上面的代码，我们成功初始化了 Softmax 回归模型的参数。让我详细解释输出结果的含义：\n",
    "\n",
    "#### 🎯 参数初始化核心信息解读\n",
    "\n",
    "**权重矩阵 W 的形状: $(784, 10)$**\n",
    "\n",
    "- **维度含义**：\n",
    "  - **784**：输入特征维度，对应展平后的图像像素数 $(28 \\times 28 = 784)$\n",
    "  - **10**：输出类别数，对应 Fashion-MNIST 的 10 个服装类别\n",
    "- **数学表示**：$\\mathbf{W} \\in \\mathbb{R}^{784 \\times 10}$\n",
    "- **物理意义**：每一列代表一个类别的特征权重，每一行代表一个像素位置对所有类别的贡献\n",
    "\n",
    "**偏置向量 b 的形状: $(10,)$**\n",
    "\n",
    "- **维度含义**：10 个偏置值，每个输出类别对应一个偏置\n",
    "- **数学表示**：$\\mathbf{b} \\in \\mathbb{R}^{10}$\n",
    "- **物理意义**：为每个类别提供基准调整值，类似于线性回归中的截距项\n",
    "\n",
    "**总参数数量: 7,850**\n",
    "\n",
    "- **计算过程**：\n",
    "  $$\\text{总参数} = |\\mathbf{W}| + |\\mathbf{b}| = 784 \\times 10 + 10 = 7,840 + 10 = 7,850$$\n",
    "- **参数分布**：\n",
    "  - **权重参数**：$7,840$ 个 $(99.87\\%)$\n",
    "  - **偏置参数**：$10$ 个 $(0.13\\%)$\n",
    "\n",
    "#### 🔧 参数初始化策略解读\n",
    "\n",
    "**权重矩阵 W 初始化：小随机数**\n",
    "\n",
    "- **方法**：从正态分布 $\\mathcal{N}(0, 0.01^2)$ 中采样\n",
    "- **数学表示**：$W_{ij} \\sim \\mathcal{N}(0, 0.01^2)$\n",
    "- **为什么用小随机数**：\n",
    "  1. **打破对称性**：如果所有权重都相同，神经元会学到相同的特征\n",
    "  2. **避免梯度爆炸**：初始值太大会导致梯度计算时数值不稳定\n",
    "  3. **保证收敛**：合适的初始化有助于梯度下降找到好的解\n",
    "\n",
    "**偏置向量 b 初始化：零初始化**\n",
    "\n",
    "- **方法**：所有偏置初始化为 $0$\n",
    "- **数学表示**：$b_i = 0, \\forall i \\in \\{1, 2, ..., 10\\}$\n",
    "- **为什么初始化为 0**：\n",
    "  1. **无偏假设**：开始时不对任何类别有偏好\n",
    "  2. **简单有效**：零初始化对偏置项来说是安全且有效的选择\n",
    "  3. **对称性考虑**：让模型在训练初期对所有类别平等对待\n",
    "\n",
    "#### 📊 内存占用分析\n",
    "\n",
    "**权重矩阵内存占用**：\n",
    "$$\\text{内存}_{W} = 784 \\times 10 \\times 4\\text{字节} = 31,360\\text{字节} = 30.625\\text{KB}$$\n",
    "\n",
    "**偏置向量内存占用**：\n",
    "$$\\text{内存}_{b} = 10 \\times 4\\text{字节} = 40\\text{字节}$$\n",
    "\n",
    "**总内存占用**：\n",
    "$$\\text{总内存} = 30.625\\text{KB} + 40\\text{字节} \\approx 30.665\\text{KB}$$\n",
    "\n",
    "#### 🧠 梯度计算准备\n",
    "\n",
    "**requires_grad=True 的重要性**：\n",
    "\n",
    "- **启用自动微分**：PyTorch 会自动追踪这些参数的计算图\n",
    "- **支持反向传播**：训练时可以自动计算梯度\n",
    "- **参数更新**：优化器可以根据梯度更新参数值\n",
    "\n",
    "**梯度计算过程**：\n",
    "\n",
    "1. **前向传播**：$\\mathbf{o} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$\n",
    "2. **损失计算**：$L = \\text{CrossEntropy}(\\text{softmax}(\\mathbf{o}), \\mathbf{y})$\n",
    "3. **反向传播**：$\\frac{\\partial L}{\\partial \\mathbf{W}}, \\frac{\\partial L}{\\partial \\mathbf{b}}$\n",
    "\n",
    "#### 💡 初始化效果验证\n",
    "\n",
    "**检查初始化的合理性**：\n",
    "\n",
    "```python\n",
    "# 权重矩阵的统计信息\n",
    "print(f\"W的均值: {W.mean().item():.6f}\")\n",
    "print(f\"W的标准差: {W.std().item():.6f}\")\n",
    "print(f\"W的最小值: {W.min().item():.6f}\")\n",
    "print(f\"W的最大值: {W.max().item():.6f}\")\n",
    "```\n",
    "\n",
    "**理想的初始化效果**：\n",
    "\n",
    "- 权重均值接近 $0$\n",
    "- 权重标准差接近 $0.01$\n",
    "- 没有异常的极大或极小值\n",
    "- 偏置全部为 $0$\n",
    "\n",
    "#### 🎯 关键要点总结\n",
    "\n",
    "1. **参数规模合理**：$7,850$ 个参数对于 Fashion-MNIST 来说是适中的规模\n",
    "2. **内存占用小**：总共只需要约 $30.7\\text{KB}$ 内存存储参数\n",
    "3. **初始化策略**：权重用小随机数，偏置用零初始化，这是经典且有效的策略\n",
    "4. **梯度就绪**：所有参数都启用了梯度计算，为训练做好准备\n",
    "5. **架构清晰**：简单的线性变换 + Softmax，易于理解和实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea92ba",
   "metadata": {},
   "source": [
    "### 3.3 定义 softmax 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e24c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个2x3的二维张量X，包含指定的浮点数值\n",
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# 计算张量X在第0维度（列）上的和，保持维度不变（结果张量的维度与输入张量相同，但第0维度大小为1）\n",
    "result_dim0 = X.sum(0, keepdim=True)\n",
    "\n",
    "# 计算张量X在第1维度（行）上的和，保持维度不变（结果张量的维度与输入张量相同，但第1维度大小为1）\n",
    "result_dim1 = X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e65d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量求和操作示例\n",
      "==============================\n",
      "原始张量 X:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "按列求和: tensor([[5., 7., 9.]])\n",
      "按行求和: tensor([[ 6.],\n",
      "        [15.]])\n",
      "这个操作在Softmax中用于计算归一化常数\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 7., 9.]]),\n",
       " tensor([[ 6.],\n",
       "         [15.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"张量求和操作示例\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"原始张量 X:\\n{X}\")\n",
    "print(f\"按列求和: {result_dim0}\")\n",
    "print(f\"按行求和: {result_dim1}\")\n",
    "print(\"这个操作在Softmax中用于计算归一化常数\")\n",
    "\n",
    "(result_dim0, result_dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b74b09",
   "metadata": {},
   "source": [
    "### 张量求和操作详解\n",
    "\n",
    "通过上面的代码，我们演示了张量求和操作，这是实现 Softmax 函数的关键步骤。让我详细解释输出结果的含义：\n",
    "\n",
    "#### 🎯 张量求和操作核心信息解读\n",
    "\n",
    "**原始张量 X:**\n",
    "\n",
    "```\n",
    "tensor([[1., 2., 3.],\n",
    "        [4., 5., 6.]])\n",
    "```\n",
    "\n",
    "- **形状**: $(2, 3)$ - 2 行 3 列的矩阵\n",
    "- **数学表示**: $\\mathbf{X} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
    "- **含义**: 可以理解为 2 个样本，每个样本有 3 个特征\n",
    "\n",
    "**按列求和 (dim=0): tensor([[5., 7., 9.]])**\n",
    "\n",
    "- **操作**: `X.sum(0, keepdim=True)`\n",
    "- **数学表示**: $\\sum_{i=0}^{1} X_{i,j}$ 对每列 $j$\n",
    "- **具体计算**:\n",
    "  - 第 1 列: $1 + 4 = 5$\n",
    "  - 第 2 列: $2 + 5 = 7$\n",
    "  - 第 3 列: $3 + 6 = 9$\n",
    "- **结果形状**: $(1, 3)$ - 保持维度（keepdim=True）\n",
    "\n",
    "**按行求和 (dim=1): tensor([[6.], [15.]])**\n",
    "\n",
    "- **操作**: `X.sum(1, keepdim=True)`\n",
    "- **数学表示**: $\\sum_{j=0}^{2} X_{i,j}$ 对每行 $i$\n",
    "- **具体计算**:\n",
    "  - 第 1 行: $1 + 2 + 3 = 6$\n",
    "  - 第 2 行: $4 + 5 + 6 = 15$\n",
    "- **结果形状**: $(2, 1)$ - 保持维度（keepdim=True）\n",
    "\n",
    "#### 🧮 在 Softmax 函数中的应用\n",
    "\n",
    "**Softmax 归一化常数计算**：\n",
    "\n",
    "在 Softmax 函数中，我们需要计算每行的指数和作为归一化常数：\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{o})_{i,j} = \\frac{\\exp(o_{i,j})}{\\sum_{k=1}^{q} \\exp(o_{i,k})}$$\n",
    "\n",
    "其中分母 $\\sum_{k=1}^{q} \\exp(o_{i,k})$ 就是按行求和操作：\n",
    "\n",
    "```python\n",
    "# 假设 o 是 logits（线性输出）\n",
    "exp_o = torch.exp(o)                    # 对每个元素取指数\n",
    "sum_exp_o = exp_o.sum(1, keepdim=True)  # 按行求和，得到归一化常数\n",
    "softmax_result = exp_o / sum_exp_o      # 每个元素除以对应行的和\n",
    "```\n",
    "\n",
    "#### 📊 keepdim 参数的重要性\n",
    "\n",
    "**keepdim=True 的作用**：\n",
    "\n",
    "- **保持维度**: 确保求和后张量维度与原张量兼容\n",
    "- **支持广播**: 便于后续的除法运算\n",
    "- **避免形状错误**: 防止维度不匹配导致的运算错误\n",
    "\n",
    "**对比示例**：\n",
    "\n",
    "```python\n",
    "# 不保持维度\n",
    "X.sum(1, keepdim=False)  # 形状: (2,)\n",
    "# 保持维度\n",
    "X.sum(1, keepdim=True)   # 形状: (2, 1)\n",
    "```\n",
    "\n",
    "#### 🔄 在 Softmax 实现中的具体应用\n",
    "\n",
    "**完整的 Softmax 实现过程**：\n",
    "\n",
    "1. **线性变换**: $\\mathbf{O} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$\n",
    "2. **取指数**: $\\mathbf{E} = \\exp(\\mathbf{O})$\n",
    "3. **按行求和**: $\\mathbf{S} = \\mathbf{E}.\\text{sum}(1, \\text{keepdim}=\\text{True})$\n",
    "4. **归一化**: $\\hat{\\mathbf{Y}} = \\mathbf{E} / \\mathbf{S}$\n",
    "\n",
    "**数学公式表示**：\n",
    "\n",
    "$$\\hat{y}_{i,j} = \\frac{\\exp(o_{i,j})}{\\sum_{k=1}^{q} \\exp(o_{i,k})}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $i$ 表示批次中的第 $i$ 个样本\n",
    "- $j$ 表示第 $j$ 个类别\n",
    "- $q$ 表示类别总数\n",
    "\n",
    "#### 💡 维度理解要点\n",
    "\n",
    "**批次处理的维度变化**：\n",
    "\n",
    "- **输入**: $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ （N 个样本，d 个特征）\n",
    "- **线性输出**: $\\mathbf{O} \\in \\mathbb{R}^{N \\times q}$ （N 个样本，q 个类别）\n",
    "- **指数**: $\\mathbf{E} \\in \\mathbb{R}^{N \\times q}$\n",
    "- **行求和**: $\\mathbf{S} \\in \\mathbb{R}^{N \\times 1}$ （每行一个归一化常数）\n",
    "- **Softmax 结果**: $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{N \\times q}$ （每行概率和为 1）\n",
    "\n",
    "### 🎯 关键要点总结\n",
    "\n",
    "1. **求和维度**: `dim=0`按列求和，`dim=1`按行求和\n",
    "2. **keepdim 作用**: 保持张量维度，支持后续广播运算\n",
    "3. **Softmax 核心**: 按行求和计算归一化常数是关键步骤\n",
    "4. **批次处理**: 每个样本独立计算 Softmax，但可以并行处理\n",
    "5. **数值稳定**: 实际实现中会使用数值稳定的版本避免溢出\n",
    "\n",
    "#### 🧠 深度理解\n",
    "\n",
    "**为什么按行求和？**\n",
    "\n",
    "在分类任务中：\n",
    "\n",
    "- 每一行代表一个样本对所有类别的\"得分\"\n",
    "- 我们需要将这些得分转换为概率分布\n",
    "- 概率分布要求：所有概率非负且和为 1\n",
    "- 因此需要按行归一化，即按行求和作为分母\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ee88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    计算softmax函数的输出。\n",
    "\n",
    "    softmax函数是机器学习中常用的激活函数，特别是在分类任务的输出层。\n",
    "    它将输入的向量转换为另一个向量，其中元素为非负且和为1，可以解释为概率分布。\n",
    "\n",
    "    参数:\n",
    "    X: 输入的张量，通常是模型的输出。\n",
    "\n",
    "    返回:\n",
    "    softmax函数的输出，每个元素表示相应类别在输入下的相对概率。\n",
    "    \"\"\"\n",
    "    # 对输入X进行指数运算，使得所有元素变为正数且增加了模型输出的非线性\n",
    "    X_exp = torch.exp(X)\n",
    "    # 在指数运算后，为了归一化概率，需要计算每个样本在各个类别上的概率之和\n",
    "    # 这里使用sum函数沿着类别维度求和，keepdim=True保持了输出的维度与输入相同，便于后续的除法操作\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    # 利用广播机制，将每个元素除以所在样本的概率之和，实现概率归一化\n",
    "    # 广播机制使得不同形状的张量能够进行运算，这里自动扩展了partition张量\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c31c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (2, 5))\n",
    "X_prob = softmax(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d575c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax函数测试\n",
      "=========================\n",
      "输入 (原始分数):\n",
      "tensor([[ 1.6142,  0.4090,  0.7797,  0.2213, -0.7567],\n",
      "        [-1.0409, -0.1811,  1.5252, -0.4980,  0.4804]])\n",
      "Softmax输出 (概率):\n",
      "tensor([[0.4818, 0.1444, 0.2092, 0.1197, 0.0450],\n",
      "        [0.0441, 0.1042, 0.5739, 0.0759, 0.2019]])\n",
      "每行概率和: tensor([1.0000, 1.0000])\n",
      "✓ 每行概率和都等于1，满足概率分布要求\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4818, 0.1444, 0.2092, 0.1197, 0.0450],\n",
       "         [0.0441, 0.1042, 0.5739, 0.0759, 0.2019]]),\n",
       " tensor([1.0000, 1.0000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Softmax函数测试\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"输入 (原始分数):\\n{X}\")\n",
    "print(f\"Softmax输出 (概率):\\n{X_prob}\")\n",
    "print(f\"每行概率和: {X_prob.sum(1)}\")\n",
    "print(\"✓ 每行概率和都等于1，满足概率分布要求\")\n",
    "\n",
    "(X_prob, X_prob.sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca5bfc",
   "metadata": {},
   "source": [
    "### Softmax函数测试结果详解\n",
    "\n",
    "通过上面的代码，我们成功实现并测试了 Softmax 函数。让我详细解释输出结果的含义：\n",
    "\n",
    "#### 🎯 Softmax函数测试核心信息解读\n",
    "\n",
    "**输入 (原始分数):**\n",
    "\n",
    "```\n",
    "tensor([[ 0.0778, -1.6418,  0.0830,  1.8546, -1.0092],\n",
    "        [-0.2616, -0.7553,  0.3474,  0.1431,  0.8178]])\n",
    "```\n",
    "\n",
    "- **数据形状**: $(2, 5)$ - 2 个样本，每个样本有 5 个类别的原始分数\n",
    "- **数值特点**: 原始分数可以是任意实数（正数、负数、零）\n",
    "- **数学表示**: $\\mathbf{X} \\in \\mathbb{R}^{2 \\times 5}$\n",
    "- **实际含义**: 这些分数通常是神经网络线性层的输出（logits）\n",
    "\n",
    "**样本分析**:\n",
    "- **第1个样本**: $[0.0778, -1.6418, 0.0830, 1.8546, -1.0092]$\n",
    "  - 最大分数: $1.8546$ (第4个类别)\n",
    "  - 最小分数: $-1.6418$ (第2个类别)\n",
    "- **第2个样本**: $[-0.2616, -0.7553, 0.3474, 0.1431, 0.8178]$\n",
    "  - 最大分数: $0.8178$ (第5个类别)\n",
    "  - 最小分数: $-0.7553$ (第2个类别)\n",
    "\n",
    "#### 📊 Softmax输出 (概率) 详解\n",
    "\n",
    "**概率分布结果:**\n",
    "\n",
    "```\n",
    "tensor([[0.1186, 0.0212, 0.1192, 0.7010, 0.0400],\n",
    "        [0.1267, 0.0774, 0.2330, 0.1900, 0.3729]])\n",
    "```\n",
    "\n",
    "**第1个样本的概率分布**:\n",
    "- **类别1**: $0.1186$ (11.86%)\n",
    "- **类别2**: $0.0212$ (2.12%) ← **最低概率**\n",
    "- **类别3**: $0.1192$ (11.92%)\n",
    "- **类别4**: $0.7010$ (70.10%) ← **最高概率**\n",
    "- **类别5**: $0.0400$ (4.00%)\n",
    "\n",
    "**第2个样本的概率分布**:\n",
    "- **类别1**: $0.1267$ (12.67%)\n",
    "- **类别2**: $0.0774$ (7.74%)\n",
    "- **类别3**: $0.2330$ (23.30%)\n",
    "- **类别4**: $0.1900$ (19.00%)\n",
    "- **类别5**: $0.3729$ (37.29%) ← **最高概率**\n",
    "\n",
    "#### 🧮 Softmax变换的数学原理验证\n",
    "\n",
    "**变换规律验证**:\n",
    "\n",
    "1. **单调性保持**: 原始分数的大小关系在概率中得到保持\n",
    "   - 第1样本: $1.8546$ (最大) → $0.7010$ (最大概率)\n",
    "   - 第1样本: $-1.6418$ (最小) → $0.0212$ (最小概率)\n",
    "\n",
    "2. **指数放大效应**: Softmax 会放大原始分数之间的差异\n",
    "   - 原始分数差: $1.8546 - (-1.6418) = 3.4964$\n",
    "   - 概率比: $0.7010 / 0.0212 = 33.07$ (放大了差异)\n",
    "\n",
    "**详细计算过程** (以第1个样本为例):\n",
    "\n",
    "$$\\text{Step 1: 计算指数}$$\n",
    "$$\\exp([0.0778, -1.6418, 0.0830, 1.8546, -1.0092])$$\n",
    "$$= [1.081, 0.194, 1.087, 6.390, 0.364]$$\n",
    "\n",
    "$$\\text{Step 2: 计算归一化常数}$$\n",
    "$$\\text{sum} = 1.081 + 0.194 + 1.087 + 6.390 + 0.364 = 9.116$$\n",
    "\n",
    "$$\\text{Step 3: 归一化得到概率}$$\n",
    "$$\\text{softmax} = \\frac{[1.081, 0.194, 1.087, 6.390, 0.364]}{9.116}$$\n",
    "$$= [0.1186, 0.0212, 0.1192, 0.7010, 0.0400]$$\n",
    "\n",
    "#### ✅ 概率分布验证\n",
    "\n",
    "**每行概率和: tensor([1.0000, 1.0000])**\n",
    "\n",
    "- **数学验证**: $\\sum_{j=1}^{5} \\hat{y}_{i,j} = 1$ 对所有 $i$\n",
    "- **第1个样本**: $0.1186 + 0.0212 + 0.1192 + 0.7010 + 0.0400 = 1.0000$ ✓\n",
    "- **第2个样本**: $0.1267 + 0.0774 + 0.2330 + 0.1900 + 0.3729 = 1.0000$ ✓\n",
    "- **结论**: 满足概率分布的基本要求\n",
    "\n",
    "#### 🔍 深度分析: Softmax的特性展现\n",
    "\n",
    "**1. 概率性质**:\n",
    "- ✅ 所有概率值都在 $[0,1]$ 区间内\n",
    "- ✅ 每行概率和为 $1$\n",
    "- ✅ 构成有效的概率分布\n",
    "\n",
    "**2. 决策特性**:\n",
    "- **第1个样本**: 模型倾向于预测第4类 (70.10%的信心)\n",
    "- **第2个样本**: 模型倾向于预测第5类 (37.29%的信心，但不如第1个样本确定)\n",
    "\n",
    "**3. 不确定性衡量**:\n",
    "- **第1个样本**: 熵较低，预测较为确定\n",
    "- **第2个样本**: 熵较高，预测不确定性较大\n",
    "\n",
    "**信息熵计算** (衡量不确定性):\n",
    "$$H_1 = -\\sum_{j=1}^{5} p_j \\log p_j \\approx 1.095 \\text{ (较低)}$$\n",
    "$$H_2 = -\\sum_{j=1}^{5} p_j \\log p_j \\approx 1.485 \\text{ (较高)}$$\n",
    "\n",
    "#### 💡 实际应用意义\n",
    "\n",
    "**在分类任务中的应用**:\n",
    "\n",
    "1. **预测类别**: 选择概率最大的类别作为预测结果\n",
    "   - 第1个样本 → 预测类别4\n",
    "   - 第2个样本 → 预测类别5\n",
    "\n",
    "2. **置信度评估**: 最大概率值反映模型的置信度\n",
    "   - 第1个样本: 70.10% 置信度 (较高)\n",
    "   - 第2个样本: 37.29% 置信度 (中等)\n",
    "\n",
    "3. **损失计算**: 这些概率将用于计算交叉熵损失\n",
    "   $$L = -\\sum_{i,j} y_{i,j} \\log(\\hat{y}_{i,j})$$\n",
    "\n",
    "#### 🎯 关键要点总结\n",
    "\n",
    "1. **Softmax成功**: 将任意实数转换为有效概率分布\n",
    "2. **单调性保持**: 原始分数的相对大小关系得到保留\n",
    "3. **差异放大**: 指数函数放大了原始分数之间的差异\n",
    "4. **概率归一**: 每行概率和严格等于1，满足概率公理\n",
    "5. **决策就绪**: 输出可直接用于分类决策和损失计算\n",
    "\n",
    "#### 🧠 数值稳定性说明\n",
    "\n",
    "虽然当前示例运行正常，但实际应用中应使用数值稳定的 Softmax 实现：\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i - \\max(\\mathbf{x}))}{\\sum_{j} \\exp(x_j - \\max(\\mathbf{x}))}$$\n",
    "\n",
    "这样可以避免指数函数溢出的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d744e2",
   "metadata": {},
   "source": [
    "## 3.5 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1312c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    \"\"\"\n",
    "    实现了一个简单的全连接神经网络层。\n",
    "\n",
    "    该函数接收一个输入张量X，并应用仿射变换（即线性变换），最后通过softmax函数进行归一化。\n",
    "    主要步骤包括：\n",
    "    1. 将输入X重塑为二维张量，其中行数为W的行数，-1表示自动推导。\n",
    "    2. 对重塑后的X进行矩阵乘法操作，乘以权重矩阵W。\n",
    "    3. 加上偏置项b，完成仿射变换。\n",
    "    4. 应用softmax函数，将输出转化为概率分布形式。\n",
    "\n",
    "    注意：此函数依赖于外部定义的权重矩阵W和偏置项b，以及softmax函数的实现。\n",
    "\n",
    "    参数:\n",
    "    X (torch.Tensor): 输入张量，可以是一批数据。\n",
    "\n",
    "    返回:\n",
    "    torch.Tensor: 经过全连接层处理后的输出张量，代表每个样本在各个类别上的概率。\n",
    "    \"\"\"\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afff60",
   "metadata": {},
   "source": [
    "## 3.6 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe46d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化真实标签y，这里使用one-hot编码方式\n",
    "y = torch.tensor([0, 2])\n",
    "\n",
    "# 初始化预测标签y_hat，这里使用softmax输出的概率分布\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2f2ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077411fb",
   "metadata": {},
   "source": [
    "### 3.4.1 高级索引操作详解\n",
    "\n",
    "通过上面的代码，我们演示了 PyTorch 的高级索引操作，这是实现交叉熵损失函数的关键步骤。让我详细解释输出结果的含义：\n",
    "\n",
    "#### 🎯 高级索引操作核心信息解读\n",
    "\n",
    "**输入数据回顾**:\n",
    "\n",
    "```python\n",
    "y = torch.tensor([0, 2])  # 真实标签\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])  # 预测概率\n",
    "```\n",
    "\n",
    "- **真实标签 y**: `[0, 2]`\n",
    "\n",
    "  - 第 1 个样本的真实类别: **类别 0**\n",
    "  - 第 2 个样本的真实类别: **类别 2**\n",
    "\n",
    "- **预测概率 y_hat**: 形状为 $(2, 3)$ 的概率矩阵\n",
    "  ```\n",
    "  [[0.1, 0.3, 0.6],   ← 第1个样本对3个类别的概率\n",
    "   [0.3, 0.2, 0.5]]   ← 第2个样本对3个类别的概率\n",
    "  ```\n",
    "\n",
    "**高级索引操作**: `y_hat[[0, 1], y]`\n",
    "\n",
    "**输出结果**: `tensor([0.1000, 0.5000])`\n",
    "\n",
    "#### 🔍 高级索引操作逐步解析\n",
    "\n",
    "**索引语法解释**:\n",
    "\n",
    "- **`y_hat[[0, 1], y]`** 等价于 **`y_hat[[0, 1], [0, 2]]`**\n",
    "- 这是**多维索引**的高级用法，同时指定行索引和列索引\n",
    "\n",
    "**逐步分解**:\n",
    "\n",
    "1. **行索引**: `[0, 1]` - 选择第 0 行和第 1 行（即所有样本）\n",
    "2. **列索引**: `y = [0, 2]` - 分别选择第 0 列和第 2 列\n",
    "3. **配对选择**:\n",
    "   - 从第 0 行选择第 0 列: `y_hat[0, 0] = 0.1`\n",
    "   - 从第 1 行选择第 2 列: `y_hat[1, 2] = 0.5`\n",
    "\n",
    "**数学表示**:\n",
    "\n",
    "$$\\text{selected\\_probs}_i = \\hat{y}_{i, y_i}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $i$ 是样本索引（0, 1）\n",
    "- $y_i$ 是第 $i$ 个样本的真实类别标签\n",
    "- $\\hat{y}_{i, y_i}$ 是第 $i$ 个样本在真实类别上的预测概率\n",
    "\n",
    "#### 📊 结果含义详解\n",
    "\n",
    "**输出值解释**:\n",
    "\n",
    "- **`0.1000`**: 第 1 个样本在**真实类别 0**上的预测概率\n",
    "\n",
    "  - 模型认为第 1 个样本属于类别 0 的概率是 **10%**\n",
    "  - 这是一个**较低的概率**，说明模型预测不够准确\n",
    "\n",
    "- **`0.5000`**: 第 2 个样本在**真实类别 2**上的预测概率\n",
    "  - 模型认为第 2 个样本属于类别 2 的概率是 **50%**\n",
    "  - 这是一个**中等的概率**，说明模型有一定的预测能力\n",
    "\n",
    "#### 🧮 在交叉熵损失中的应用\n",
    "\n",
    "**交叉熵损失公式**:\n",
    "\n",
    "$$L = -\\sum_{i=1}^{N} \\log(\\hat{y}_{i, y_i})$$\n",
    "\n",
    "**具体计算过程**:\n",
    "\n",
    "1. **选取真实类别概率**: `y_hat[[0, 1], y] = [0.1, 0.5]`\n",
    "2. **计算对数**: `torch.log([0.1, 0.5]) = [-2.3026, -0.6931]`\n",
    "3. **取负号**: `-torch.log([0.1, 0.5]) = [2.3026, 0.6931]`\n",
    "4. **损失值**: 每个样本的交叉熵损失\n",
    "\n",
    "**损失值意义**:\n",
    "\n",
    "- **第 1 个样本损失**: $-\\log(0.1) = 2.3026$ (较大损失)\n",
    "  - 预测概率低，惩罚重\n",
    "- **第 2 个样本损失**: $-\\log(0.5) = 0.6931$ (较小损失)\n",
    "  - 预测概率高，惩罚轻\n",
    "\n",
    "#### 💡 高级索引的优势\n",
    "\n",
    "**与传统方法对比**:\n",
    "\n",
    "```python\n",
    "# 传统方法 (低效)\n",
    "selected_probs = []\n",
    "for i in range(len(y)):\n",
    "    selected_probs.append(y_hat[i, y[i]])\n",
    "\n",
    "# 高级索引方法 (高效)\n",
    "selected_probs = y_hat[range(len(y)), y]  # 或 y_hat[[0, 1], y]\n",
    "```\n",
    "\n",
    "**优势分析**:\n",
    "\n",
    "1. **代码简洁**: 一行代码完成复杂的索引操作\n",
    "2. **计算高效**: 向量化操作，无需循环\n",
    "3. **内存友好**: 避免中间变量的创建\n",
    "4. **批处理**: 可以同时处理整个批次的数据\n",
    "\n",
    "#### 🔄 通用化表示\n",
    "\n",
    "**通用公式** (适用于任意批次大小):\n",
    "\n",
    "```python\n",
    "# 对于批次大小为 N 的数据\n",
    "selected_probs = y_hat[range(N), y]\n",
    "```\n",
    "\n",
    "**等价写法**:\n",
    "\n",
    "```python\n",
    "# 方法1: 使用 range\n",
    "y_hat[range(len(y)), y]\n",
    "\n",
    "# 方法2: 使用具体索引 (仅适用于已知大小)\n",
    "y_hat[[0, 1], y]  # 当批次大小为2时\n",
    "\n",
    "# 方法3: 使用 torch.gather\n",
    "torch.gather(y_hat, 1, y.unsqueeze(1)).squeeze(1)\n",
    "```\n",
    "\n",
    "#### 🎯 关键要点总结\n",
    "\n",
    "1. **高级索引**: `y_hat[[0, 1], y]` 实现了高效的概率选择\n",
    "2. **结果含义**: 返回每个样本在其真实类别上的预测概率\n",
    "3. **损失计算**: 这些概率是计算交叉熵损失的关键输入\n",
    "4. **向量化**: 避免了循环，提高了计算效率\n",
    "5. **可扩展性**: 可以轻松扩展到任意批次大小\n",
    "\n",
    "#### 🧠 深度理解\n",
    "\n",
    "**为什么选择真实类别的概率？**\n",
    "\n",
    "在**监督学习**中：\n",
    "\n",
    "- 我们只关心模型在**真实类别**上的预测表现\n",
    "- 真实类别的概率越高，说明模型预测越准确\n",
    "- 交叉熵损失通过**负对数**将概率转换为损失值\n",
    "- 概率高 → 损失小，概率低 → 损失大\n",
    "\n",
    "**实际应用场景**:\n",
    "\n",
    "- **模型评估**: 检查模型在正确类别上的置信度\n",
    "- **损失计算**: 交叉熵损失函数的核心步骤\n",
    "- **梯度计算**: 反向传播时计算梯度的基础\n",
    "\n",
    "### 3.6 定义损失函数\n",
    "\n",
    "在 PyTorch 中，损失函数用于衡量模型预测值与真实值之间的差距。定义损失函数的步骤如下：\n",
    "\n",
    "#### 步骤 1: 选择损失函数类型\n",
    "\n",
    "根据任务需求选择合适的损失函数，例如：\n",
    "\n",
    "- **回归任务**: 均方误差损失 `torch.nn.MSELoss`\n",
    "- **二分类任务**: 二元交叉熵损失 `torch.nn.BCELoss`\n",
    "- **多分类任务**: 交叉熵损失 `torch.nn.CrossEntropyLoss`\n",
    "\n",
    "#### 步骤 2: 初始化损失函数\n",
    "\n",
    "以交叉熵损失为例，进行初始化设置：\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# 初始化交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "#### 步骤 3: 前向传播计算损失\n",
    "\n",
    "在模型前向传播过程中计算损失值：\n",
    "\n",
    "```python\n",
    "# 假设 y_pred 为模型预测值，y_true 为真实标签\n",
    "loss = criterion(y_pred, y_true)\n",
    "```\n",
    "\n",
    "#### 步骤 4: 反向传播更新参数\n",
    "\n",
    "通过反向传播算法更新模型参数：\n",
    "\n",
    "```python\n",
    "# 清零梯度\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 更新参数\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "#### 示例代码\n",
    "\n",
    "以下是一个完整的示例代码，演示如何定义损失函数并进行模型训练：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 假设有一个简单的神经网络模型\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2)\n",
    ")\n",
    "\n",
    "# 定义损失函数为交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 假设有输入数据 x 和对应的真实标签 y\n",
    "x = torch.randn(10)\n",
    "y = torch.tensor([1])\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output.unsqueeze(0), y)\n",
    "\n",
    "# 反向传播\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "#### 注意事项\n",
    "\n",
    "- 确保输入数据和标签的形状与损失函数要求一致\n",
    "- 在多分类任务中，交叉熵损失的输入通常是未经过 softmax 函数的原始 logits\n",
    "- 使用 `torch.argmax` 获取预测类别时，需指定 `dim` 参数，例如：`torch.argmax(output, dim=1)`\n",
    "\n",
    "#### 小结\n",
    "\n",
    "- 本节介绍了如何在 PyTorch 中定义损失函数\n",
    "- 以交叉熵损失为例，演示了损失函数的初始化、前向传播计算损失以及反向传播更新参数的过程\n",
    "- 提供了一个完整的示例代码，帮助理解损失函数在模型训练中的应用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df650943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "\n",
    "loss_values = cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d1e5a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉熵损失计算\n",
      "=========================\n",
      "选中概率: tensor([0.1000, 0.5000])\n",
      "交叉熵损失: tensor([2.3026, 0.6931])\n",
      "解释: 损失 = -log(真实类别的预测概率)\n",
      "概率越高，损失越小；概率越低，损失越大\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"交叉熵损失计算\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"选中概率: {y_hat[range(len(y_hat)), y]}\")\n",
    "print(f\"交叉熵损失: {loss_values}\")\n",
    "print(\"解释: 损失 = -log(真实类别的预测概率)\")\n",
    "print(\"概率越高，损失越小；概率越低，损失越大\")\n",
    "\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683548f",
   "metadata": {},
   "source": [
    "### 3.5 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3ff8e",
   "metadata": {},
   "source": [
    "### 📊 交叉熵损失计算结果详解\n",
    "\n",
    "#### 🎯 输出结果解析\n",
    "\n",
    "**原始输出**:\n",
    "\n",
    "```\n",
    "选中概率: tensor([0.1000, 0.5000])\n",
    "交叉熵损失: tensor([2.3026, 0.6931])\n",
    "```\n",
    "\n",
    "#### 🧮 交叉熵损失计算原理\n",
    "\n",
    "**数学公式**:\n",
    "\n",
    "$$L_i = -\\log(p_{i,y_i})$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $L_i$ 是第 $i$ 个样本的交叉熵损失\n",
    "- $p_{i,y_i}$ 是第 $i$ 个样本在真实类别 $y_i$ 上的预测概率\n",
    "\n",
    "#### 📈 逐样本损失分析\n",
    "\n",
    "**第 1 个样本**:\n",
    "\n",
    "- **选中概率**: `0.1000` (10%)\n",
    "- **损失计算**: $-\\log(0.1) = 2.3026$\n",
    "- **损失含义**:\n",
    "  - 预测概率很低，模型对真实类别不够 confident\n",
    "  - **高损失值**表示预测效果差，需要更多训练\n",
    "\n",
    "**第 2 个样本**:\n",
    "\n",
    "- **选中概率**: `0.5000` (50%)\n",
    "- **损失计算**: $-\\log(0.5) = 0.6931$\n",
    "- **损失含义**:\n",
    "  - 预测概率适中，模型有一定的预测能力\n",
    "  - **中等损失值**表示预测效果一般，还有改善空间\n",
    "\n",
    "#### 🔍 损失函数特性分析\n",
    "\n",
    "**损失与概率的关系**:\n",
    "\n",
    "| 预测概率   | 损失值 | 模型表现     |\n",
    "| ---------- | ------ | ------------ |\n",
    "| 1.0 (100%) | 0.0    | 完美预测     |\n",
    "| 0.5 (50%)  | 0.693  | 随机猜测水平 |\n",
    "| 0.1 (10%)  | 2.303  | 糟糕预测     |\n",
    "| 0.01 (1%)  | 4.605  | 非常糟糕     |\n",
    "\n",
    "**关键特点**:\n",
    "\n",
    "1. **非线性惩罚**: 损失函数是凸函数，概率越低惩罚越重\n",
    "2. **无上界**: 当概率趋近于 0 时，损失趋向无穷大\n",
    "3. **梯度友好**: 对数函数的梯度有利于反向传播\n",
    "\n",
    "#### 📊 可视化理解\n",
    "\n",
    "```\n",
    "概率: 0.1  →  损失: 2.30  [████████████] 高损失\n",
    "概率: 0.5  →  损失: 0.69  [████____]     中等损失\n",
    "```\n",
    "\n",
    "**损失分布**:\n",
    "\n",
    "- **样本 1**: 损失占总体的 `2.30/(2.30+0.69) = 76.9%`\n",
    "- **样本 2**: 损失占总体的 `0.69/(2.30+0.69) = 23.1%`\n",
    "\n",
    "#### 🎯 训练指导意义\n",
    "\n",
    "**优化方向**:\n",
    "\n",
    "1. **样本 1 (损失 2.30)**:\n",
    "\n",
    "   - 需要**重点关注**，调整权重使其概率从 0.1 提升\n",
    "   - 梯度较大，权重更新幅度会更明显\n",
    "\n",
    "2. **样本 2 (损失 0.69)**:\n",
    "   - 预测相对较好，但仍有提升空间\n",
    "   - 梯度适中，稳步优化\n",
    "\n",
    "**总体策略**:\n",
    "\n",
    "- 交叉熵损失的设计使得模型会**自动聚焦**于预测错误的样本\n",
    "- 通过梯度下降，模型将逐步提高在真实类别上的预测概率\n",
    "\n",
    "#### 🔄 与其他损失函数对比\n",
    "\n",
    "**MSE vs 交叉熵**:\n",
    "\n",
    "| 损失函数 | 适用场景 | 梯度特性               |\n",
    "| -------- | -------- | ---------------------- |\n",
    "| MSE      | 回归问题 | 线性梯度               |\n",
    "| 交叉熵   | 分类问题 | 非线性梯度，自适应惩罚 |\n",
    "\n",
    "**交叉熵的优势**:\n",
    "\n",
    "- 对于错误预测，提供更强的梯度信号\n",
    "- 收敛速度通常比 MSE 更快\n",
    "- 数值稳定性更好（配合 softmax 使用时）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d4546",
   "metadata": {},
   "source": [
    "### 3.4 分类精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c812eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  # @save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98d6fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7a79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率计算\n",
      "====================\n",
      "预测类别: tensor([2, 2])\n",
      "真实类别: tensor([0, 2])\n",
      "准确率: 0.500 (50.0%)\n",
      "计算方法: 正确预测数量 / 总样本数量\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"准确率计算\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"预测类别: {y_hat.argmax(axis=1)}\")\n",
    "print(f\"真实类别: {y}\")\n",
    "print(f\"准确率: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "print(\"计算方法: 正确预测数量 / 总样本数量\")\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7f118",
   "metadata": {},
   "source": [
    "### 📊 准确率计算结果详解\n",
    "\n",
    "#### 🎯 输出结果解析\n",
    "\n",
    "**原始输出**:\n",
    "\n",
    "```\n",
    "预测类别: tensor([2, 2])\n",
    "真实类别: tensor([0, 2])\n",
    "准确率: 0.500 (50.0%)\n",
    "```\n",
    "\n",
    "#### 🧮 准确率计算原理\n",
    "\n",
    "**数学公式**:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{正确预测的样本数}}{\\text{总样本数}} = \\frac{\\sum_{i=1}^{N} \\mathbb{1}(\\hat{y}_i = y_i)}{N}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\hat{y}_i$ 是第 $i$ 个样本的预测类别\n",
    "- $y_i$ 是第 $i$ 个样本的真实类别\n",
    "- $\\mathbb{1}(\\cdot)$ 是指示函数，条件为真时返回 1，否则返回 0\n",
    "- $N$ 是总样本数\n",
    "\n",
    "#### 📈 逐样本预测分析\n",
    "\n",
    "**样本预测对比**:\n",
    "\n",
    "| 样本序号 | 预测类别 | 真实类别 | 预测结果 | 说明                             |\n",
    "| -------- | -------- | -------- | -------- | -------------------------------- |\n",
    "| 样本 1   | 2        | 0        | ❌ 错误  | 模型预测为类别 2，实际为类别 0   |\n",
    "| 样本 2   | 2        | 2        | ✅ 正确  | 模型预测为类别 2，实际也是类别 2 |\n",
    "\n",
    "**预测过程解析**:\n",
    "\n",
    "1. **样本 1**: `y_hat[0].argmax() = 2`, `y[0] = 0` → **预测错误**\n",
    "2. **样本 2**: `y_hat[1].argmax() = 2`, `y[1] = 2` → **预测正确**\n",
    "\n",
    "#### 🔍 预测类别提取过程\n",
    "\n",
    "**argmax 操作详解**:\n",
    "\n",
    "```python\n",
    "# 假设原始概率分布为：\n",
    "# y_hat = [[0.1, 0.2, 0.7],    # 样本1: 最大概率在索引2 → 预测类别2\n",
    "#          [0.3, 0.2, 0.5]]    # 样本2: 最大概率在索引2 → 预测类别2\n",
    "\n",
    "y_hat.argmax(axis=1)  # 返回每行最大值的索引\n",
    "# 结果: tensor([2, 2])\n",
    "```\n",
    "\n",
    "**关键步骤**:\n",
    "\n",
    "1. **沿着类别维度求最大值索引**: `axis=1` 表示在每个样本的类别概率中找最大值\n",
    "2. **返回预测类别**: 最大概率对应的索引即为预测的类别标签\n",
    "3. **与真实标签比较**: 逐元素比较预测类别和真实类别\n",
    "\n",
    "#### 📊 准确率计算过程\n",
    "\n",
    "**具体计算步骤**:\n",
    "\n",
    "```python\n",
    "# 1. 提取预测类别\n",
    "predicted = y_hat.argmax(axis=1)  # tensor([2, 2])\n",
    "true_labels = y                   # tensor([0, 2])\n",
    "\n",
    "# 2. 逐元素比较\n",
    "correct_predictions = (predicted == true_labels)  # tensor([False, True])\n",
    "\n",
    "# 3. 统计正确预测数量\n",
    "num_correct = correct_predictions.sum()  # tensor(1)\n",
    "\n",
    "# 4. 计算准确率\n",
    "accuracy = num_correct / len(y)  # 1 / 2 = 0.5\n",
    "```\n",
    "\n",
    "**计算结果验证**:\n",
    "\n",
    "- **正确预测数量**: 1 个（只有样本 2 预测正确）\n",
    "- **总样本数量**: 2 个\n",
    "- **准确率**: $\\frac{1}{2} = 0.5 = 50\\%$\n",
    "\n",
    "#### 🎯 模型性能评估\n",
    "\n",
    "**当前性能水平**:\n",
    "\n",
    "| 指标     | 数值 | 解释                   |\n",
    "| -------- | ---- | ---------------------- |\n",
    "| 准确率   | 50%  | 相当于随机猜测的水平   |\n",
    "| 错误率   | 50%  | 仍有一半的样本预测错误 |\n",
    "| 正确样本 | 1/2  | 只有样本 2 预测正确    |\n",
    "\n",
    "**性能基准对比**:\n",
    "\n",
    "```\n",
    "随机猜测基准    : 33.3% (3类分类问题)\n",
    "当前模型性能    : 50.0% ████████████████▌\n",
    "理想模型性能    : 100%  ████████████████████████████████\n",
    "```\n",
    "\n",
    "#### 🔄 模型改进方向\n",
    "\n",
    "**错误分析**:\n",
    "\n",
    "1. **样本 1 错误原因**:\n",
    "\n",
    "   - 真实类别：0，预测类别：2\n",
    "   - 可能原因：特征提取不充分，权重参数需要调整\n",
    "   - 改进方向：增加训练轮次，调整学习率\n",
    "\n",
    "2. **样本 2 正确预测**:\n",
    "   - 说明模型对类别 2 有一定的识别能力\n",
    "   - 可以作为正面案例分析特征模式\n",
    "\n",
    "**训练策略建议**:\n",
    "\n",
    "1. **继续训练**: 当前准确率仅 50%，需要更多训练轮次\n",
    "2. **调整超参数**: 学习率、批次大小等可能需要优化\n",
    "3. **数据增强**: 增加训练数据的多样性\n",
    "4. **特征工程**: 考虑是否需要更好的特征表示\n",
    "\n",
    "#### 💡 准确率指标的意义\n",
    "\n",
    "**优点**:\n",
    "\n",
    "- **直观易懂**: 百分比形式，容易理解\n",
    "- **快速评估**: 一个数值就能反映整体性能\n",
    "- **比较基准**: 便于不同模型间的性能比较\n",
    "\n",
    "**局限性**:\n",
    "\n",
    "- **类别不平衡敏感**: 在数据不平衡时可能误导\n",
    "- **细节信息缺失**: 无法了解具体哪些类别预测效果好/差\n",
    "- **阈值依赖**: 基于 argmax 的硬分类，不考虑概率置信度\n",
    "\n",
    "#### 🔄 下一步分析建议\n",
    "\n",
    "**深入评估**:\n",
    "\n",
    "1. **混淆矩阵**: 分析各类别间的预测混淆情况\n",
    "2. **分类报告**: 计算每个类别的精确率、召回率、F1 分数\n",
    "3. **概率分析**: 查看预测概率的分布和置信度\n",
    "4. **学习曲线**: 观察训练过程中准确率的变化趋势\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66d20ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  # @save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa3f6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  # @save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e753a76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1427"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b4c82",
   "metadata": {},
   "source": [
    "### 3.7 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d1560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  # @save\n",
    "    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用PyTorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e778a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:  # @save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xlabel=None,\n",
    "        ylabel=None,\n",
    "        legend=None,\n",
    "        xlim=None,\n",
    "        ylim=None,\n",
    "        xscale=\"linear\",\n",
    "        yscale=\"linear\",\n",
    "        fmts=(\"-\", \"m--\", \"g-.\", \"r:\"),\n",
    "        nrows=1,\n",
    "        ncols=1,\n",
    "        figsize=(3.5, 2.5),\n",
    "    ):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [\n",
    "                self.axes,\n",
    "            ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend\n",
    "        )\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe86e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  # @save\n",
    "    \"\"\"训练模型（定义见第3章）\"\"\"\n",
    "    animator = Animator(\n",
    "        xlabel=\"epoch\",\n",
    "        xlim=[1, num_epochs],\n",
    "        ylim=[0.3, 0.9],\n",
    "        legend=[\"train loss\", \"train acc\", \"test acc\"],\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bd5960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W, b], lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e0123a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-06-23T16:02:28.466577</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 238.965625 183.35625 \n",
       "L 238.965625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 145.8 \n",
       "L 225.403125 145.8 \n",
       "L 225.403125 7.2 \n",
       "L 30.103125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 51.803125 145.8 \n",
       "L 51.803125 7.2 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m80e2735e00\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80e2735e00\" x=\"51.803125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(48.621875 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 95.203125 145.8 \n",
       "L 95.203125 7.2 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80e2735e00\" x=\"95.203125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(92.021875 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 138.603125 145.8 \n",
       "L 138.603125 7.2 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80e2735e00\" x=\"138.603125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(135.421875 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 182.003125 145.8 \n",
       "L 182.003125 7.2 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80e2735e00\" x=\"182.003125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(178.821875 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 225.403125 145.8 \n",
       "L 225.403125 7.2 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m80e2735e00\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(219.040625 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(112.525 174.076563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 30.103125 122.7 \n",
       "L 225.403125 122.7 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"md6fb47e941\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md6fb47e941\" x=\"30.103125\" y=\"122.7\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 126.499219)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 30.103125 76.5 \n",
       "L 225.403125 76.5 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md6fb47e941\" x=\"30.103125\" y=\"76.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 80.299219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 30.103125 30.3 \n",
       "L 225.403125 30.3 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md6fb47e941\" x=\"30.103125\" y=\"30.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 34.099219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 30.103125 33.448842 \n",
       "L 51.803125 83.405468 \n",
       "L 73.503125 93.717409 \n",
       "L 95.203125 99.118897 \n",
       "L 116.903125 102.923895 \n",
       "L 138.603125 105.786035 \n",
       "L 160.303125 107.813945 \n",
       "L 182.003125 109.271093 \n",
       "L 203.703125 110.806848 \n",
       "L 225.403125 112.070459 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_18\">\n",
       "    <path d=\"M 30.103125 41.77685 \n",
       "L 51.803125 27.12375 \n",
       "L 73.503125 24.32095 \n",
       "L 95.203125 22.94265 \n",
       "L 116.903125 21.86465 \n",
       "L 138.603125 21.17165 \n",
       "L 160.303125 20.367 \n",
       "L 182.003125 19.9127 \n",
       "L 203.703125 19.4815 \n",
       "L 225.403125 19.13115 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_19\">\n",
       "    <path d=\"M 30.103125 33.8112 \n",
       "L 51.803125 30.6234 \n",
       "L 73.503125 25.9572 \n",
       "L 95.203125 25.6338 \n",
       "L 116.903125 26.6271 \n",
       "L 138.603125 24.8484 \n",
       "L 160.303125 30.7389 \n",
       "L 182.003125 23.3469 \n",
       "L 203.703125 25.7493 \n",
       "L 225.403125 22.8387 \n",
       "\" clip-path=\"url(#p0703385860)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 145.8 \n",
       "L 30.103125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 225.403125 145.8 \n",
       "L 225.403125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 145.8 \n",
       "L 225.403125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.2 \n",
       "L 225.403125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 140.634375 100.017188 \n",
       "L 218.403125 100.017188 \n",
       "Q 220.403125 100.017188 220.403125 98.017188 \n",
       "L 220.403125 54.982812 \n",
       "Q 220.403125 52.982812 218.403125 52.982812 \n",
       "L 140.634375 52.982812 \n",
       "Q 138.634375 52.982812 138.634375 54.982812 \n",
       "L 138.634375 98.017188 \n",
       "Q 138.634375 100.017188 140.634375 100.017188 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\">\n",
       "     <path d=\"M 142.634375 61.08125 \n",
       "L 152.634375 61.08125 \n",
       "L 162.634375 61.08125 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(170.634375 64.58125)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_21\">\n",
       "     <path d=\"M 142.634375 75.759375 \n",
       "L 152.634375 75.759375 \n",
       "L 162.634375 75.759375 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(170.634375 79.259375)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_22\">\n",
       "     <path d=\"M 142.634375 90.4375 \n",
       "L 152.634375 90.4375 \n",
       "L 162.634375 90.4375 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- test acc -->\n",
       "     <g transform=\"translate(170.634375 93.9375)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"100.732422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"152.832031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"192.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"223.828125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"285.107422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"340.087891\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p0703385860\">\n",
       "   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77eb428",
   "metadata": {},
   "source": [
    "### 3.8 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cc2fa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"520.1pt\" height=\"118.198357pt\" viewBox=\"0 0 520.1 118.198357\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-06-23T16:02:31.580449</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 118.198357 \n",
       "L 520.1 118.198357 \n",
       "L 520.1 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 10.7 107.498357 \n",
       "L 82.442857 107.498357 \n",
       "L 82.442857 35.7555 \n",
       "L 10.7 35.7555 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pc4b07a1d2f)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image0d98c93a13\" transform=\"scale(1 -1)translate(0 -72)\" x=\"10.7\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 10.7 107.498357 \n",
       "L 10.7 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 82.442857 107.498357 \n",
       "L 82.442857 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 10.7 107.498357 \n",
       "L 82.442857 107.498357 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 10.7 35.7555 \n",
       "L 82.442857 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    <!-- ankle boot -->\n",
       "    <g transform=\"translate(14.848304 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6b\" x=\"124.658203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"182.568359\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"210.351562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"271.875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" x=\"303.662109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"367.138672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"428.320312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"489.501953\"/>\n",
       "    </g>\n",
       "    <!-- ankle boot -->\n",
       "    <g transform=\"translate(14.848304 29.7555)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"61.279297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6b\" x=\"124.658203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"182.568359\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"210.351562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"271.875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" x=\"303.662109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"367.138672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"428.320312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"489.501953\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 96.791429 107.498357 \n",
       "L 168.534286 107.498357 \n",
       "L 168.534286 35.7555 \n",
       "L 96.791429 35.7555 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pfe70cf8440)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image8bc6363f2a\" transform=\"scale(1 -1)translate(0 -72)\" x=\"96.791429\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 96.791429 107.498357 \n",
       "L 96.791429 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 168.534286 107.498357 \n",
       "L 168.534286 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 96.791429 107.498357 \n",
       "L 168.534286 107.498357 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 96.791429 35.7555 \n",
       "L 168.534286 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_2\">\n",
       "    <!-- pullover -->\n",
       "    <g transform=\"translate(108.336607 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"126.855469\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"154.638672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"182.421875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"243.603516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"302.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"364.306641\"/>\n",
       "    </g>\n",
       "    <!-- pullover -->\n",
       "    <g transform=\"translate(108.336607 29.7555)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"126.855469\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"154.638672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"182.421875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"243.603516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"302.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"364.306641\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 182.882857 107.498357 \n",
       "L 254.625714 107.498357 \n",
       "L 254.625714 35.7555 \n",
       "L 182.882857 35.7555 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p356ebcbab2)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"imagefb7a3895f7\" transform=\"scale(1 -1)translate(0 -72)\" x=\"182.882857\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 182.882857 107.498357 \n",
       "L 182.882857 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 254.625714 107.498357 \n",
       "L 254.625714 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 182.882857 107.498357 \n",
       "L 254.625714 107.498357 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 182.882857 35.7555 \n",
       "L 254.625714 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_3\">\n",
       "    <!-- trouser -->\n",
       "    <g transform=\"translate(197.312723 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"78.072266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"139.253906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"202.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"254.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"316.255859\"/>\n",
       "    </g>\n",
       "    <!-- trouser -->\n",
       "    <g transform=\"translate(197.312723 29.7555)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"78.072266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"139.253906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"202.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"254.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"316.255859\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 268.974286 107.498357 \n",
       "L 340.717143 107.498357 \n",
       "L 340.717143 35.7555 \n",
       "L 268.974286 35.7555 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p6de2b9fbde)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image54ce413ac2\" transform=\"scale(1 -1)translate(0 -72)\" x=\"268.974286\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 268.974286 107.498357 \n",
       "L 268.974286 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 340.717143 107.498357 \n",
       "L 340.717143 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 268.974286 107.498357 \n",
       "L 340.717143 107.498357 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 268.974286 35.7555 \n",
       "L 340.717143 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_4\">\n",
       "    <!-- trouser -->\n",
       "    <g transform=\"translate(283.404152 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"78.072266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"139.253906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"202.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"254.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"316.255859\"/>\n",
       "    </g>\n",
       "    <!-- trouser -->\n",
       "    <g transform=\"translate(283.404152 29.7555)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"78.072266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"139.253906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"202.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"254.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"316.255859\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_5\">\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 355.065714 107.498357 \n",
       "L 426.808571 107.498357 \n",
       "L 426.808571 35.7555 \n",
       "L 355.065714 35.7555 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p1b38ae1a03)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image86582211ec\" transform=\"scale(1 -1)translate(0 -72)\" x=\"355.065714\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 355.065714 107.498357 \n",
       "L 355.065714 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 426.808571 107.498357 \n",
       "L 426.808571 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 355.065714 107.498357 \n",
       "L 426.808571 107.498357 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_26\">\n",
       "    <path d=\"M 355.065714 35.7555 \n",
       "L 426.808571 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_5\">\n",
       "    <!-- shirt -->\n",
       "    <g transform=\"translate(377.523393 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"52.099609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"115.478516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"143.261719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"184.375\"/>\n",
       "    </g>\n",
       "    <!-- shirt -->\n",
       "    <g transform=\"translate(377.523393 29.7555)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"52.099609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"115.478516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"143.261719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"184.375\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_6\">\n",
       "   <g id=\"patch_27\">\n",
       "    <path d=\"M 441.157143 107.498357 \n",
       "L 512.9 107.498357 \n",
       "L 512.9 35.7555 \n",
       "L 441.157143 35.7555 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pb3cfa51a9e)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"imagef7e74ac7cf\" transform=\"scale(1 -1)translate(0 -72)\" x=\"441.157143\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_28\">\n",
       "    <path d=\"M 441.157143 107.498357 \n",
       "L 441.157143 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_29\">\n",
       "    <path d=\"M 512.9 107.498357 \n",
       "L 512.9 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_30\">\n",
       "    <path d=\"M 441.157143 107.498357 \n",
       "L 512.9 107.498357 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_31\">\n",
       "    <path d=\"M 441.157143 35.7555 \n",
       "L 512.9 35.7555 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_6\">\n",
       "    <!-- trouser -->\n",
       "    <g transform=\"translate(455.587009 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"78.072266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"139.253906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"202.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"254.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"316.255859\"/>\n",
       "    </g>\n",
       "    <!-- trouser -->\n",
       "    <g transform=\"translate(455.587009 29.7555)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"78.072266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"139.253906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"202.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"254.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"316.255859\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pc4b07a1d2f\">\n",
       "   <rect x=\"10.7\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pfe70cf8440\">\n",
       "   <rect x=\"96.791429\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p356ebcbab2\">\n",
       "   <rect x=\"182.882857\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p6de2b9fbde\">\n",
       "   <rect x=\"268.974286\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p1b38ae1a03\">\n",
       "   <rect x=\"355.065714\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pb3cfa51a9e\">\n",
       "   <rect x=\"441.157143\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 900x150 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_ch3(net, test_iter, n=6):  #@save\n",
    "    \"\"\"预测标签（定义见第3章）\"\"\"\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    titles = [true +'\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(\n",
    "        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
